import torch
import torch.nn as nn
import torchvision.transforms as transforms
import numpy as np
from tqdm import tqdm
import json
import os
from datetime import datetime
from collections import defaultdict, Counter
import matplotlib.pyplot as plt

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
from data.voc_dataset import VOC2007Dataset
from models.rcnn import RCNN

# VOC í´ë˜ìŠ¤ ì •ì˜
VOC_CLASSES = [
    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',
    'bus', 'car', 'cat', 'chair', 'cow',
    'diningtable', 'dog', 'horse', 'motorbike', 'person',
    'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'
]

def load_from_scratch_model(model_path):
    """From Scratch ëª¨ë¸ ë¡œë“œ"""
    print(f"ğŸ”„ Loading From Scratch model: {model_path}")
    
    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    
    # ëª¨ë¸ ìƒì„± (pretrained=False)
    model = RCNN(num_classes=20, pretrained=False).to(device)
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    if os.path.exists(model_path):
        checkpoint = torch.load(model_path, map_location=device)
        
        if isinstance(checkpoint, dict):
            if 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
                print(f"âœ… Loaded model state dict from checkpoint")
            elif 'state_dict' in checkpoint:
                model.load_state_dict(checkpoint['state_dict'])
                print(f"âœ… Loaded state dict from checkpoint")
            else:
                model.load_state_dict(checkpoint)
                print(f"âœ… Loaded checkpoint directly")
        else:
            model.load_state_dict(checkpoint)
            print(f"âœ… Loaded model weights directly")
    else:
        raise FileNotFoundError(f"Model file not found: {model_path}")
    
    model.eval()
    return model, device

def evaluate_model_comprehensive(model, device, sample_size=None):
    """ì¢…í•©ì ì¸ ëª¨ë¸ í‰ê°€"""
    print("\nğŸ“Š Starting comprehensive evaluation...")
    
    # ë°ì´í„°ì…‹ ì¤€ë¹„
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ
    try:
        test_dataset = VOC2007Dataset(
            root_dir='/Users/milkylee/Documents/Projects/VOC/voc2007/VOCdevkit/VOC2007',
            image_set='test',
            transform=transform
        )
        print(f"âœ… Test dataset loaded: {len(test_dataset)} samples")
    except:
        print("âš ï¸ Test set not available, using validation set...")
        test_dataset = VOC2007Dataset(
            root_dir='/Users/milkylee/Documents/Projects/VOC/voc2007/VOCdevkit/VOC2007',
            image_set='val',
            transform=transform
        )
        print(f"âœ… Validation dataset loaded: {len(test_dataset)} samples")
    
    # ìƒ˜í”Œ í¬ê¸° ì¡°ì •
    if sample_size:
        evaluation_size = min(sample_size, len(test_dataset))
    else:
        evaluation_size = len(test_dataset)
    
    print(f"ğŸ“ Evaluating on {evaluation_size} samples")
    
    # í‰ê°€ ë©”íŠ¸ë¦­ ì´ˆê¸°í™”
    total_samples = 0
    correct_predictions = 0
    class_predictions = Counter()
    class_ground_truth = Counter()
    class_correct = defaultdict(int)
    
    # í´ë˜ìŠ¤ë³„ TP, FP, FN ì¹´ìš´í„°
    true_positives = defaultdict(int)
    false_positives = defaultdict(int)
    false_negatives = defaultdict(int)
    
    all_predictions = []
    all_ground_truths = []
    prediction_confidences = []
    
    print("\nğŸ”„ Running evaluation...")
    
    with torch.no_grad():
        for i in tqdm(range(evaluation_size), desc="Evaluating"):
            try:
                sample = test_dataset[i]
                image = sample['image'].unsqueeze(0).to(device)
                labels = sample['labels']  # ì‹¤ì œ ë ˆì´ë¸”ë“¤
                
                if len(labels) == 0:
                    continue
                
                # ëª¨ë¸ ì˜ˆì¸¡
                cls_scores, _ = model(image)
                probs = torch.softmax(cls_scores, dim=1)
                predicted_class = torch.argmax(probs, dim=1).item()
                confidence = probs.max().item()
                
                # í†µê³„ ìˆ˜ì§‘
                total_samples += 1
                prediction_confidences.append(confidence)
                all_predictions.append(predicted_class)
                all_ground_truths.append(labels)
                
                # í´ë˜ìŠ¤ë³„ ì¹´ìš´íŒ…
                class_predictions[predicted_class] += 1
                for label in labels:
                    class_ground_truth[label] += 1
                
                # ì •í™•ë„ ê³„ì‚° (ì˜ˆì¸¡í•œ í´ë˜ìŠ¤ê°€ ì‹¤ì œ ë ˆì´ë¸” ì¤‘ í•˜ë‚˜ì™€ ì¼ì¹˜í•˜ëŠ”ì§€)
                if predicted_class in labels:
                    correct_predictions += 1
                    class_correct[predicted_class] += 1
                    true_positives[predicted_class] += 1
                else:
                    false_positives[predicted_class] += 1
                
                # False Negatives ê³„ì‚°
                for label in labels:
                    if label != predicted_class:
                        false_negatives[label] += 1
                        
            except Exception as e:
                print(f"Error processing sample {i}: {e}")
                continue
    
    # ì „ì²´ ì •í™•ë„ ê³„ì‚°
    overall_accuracy = (correct_predictions / total_samples * 100) if total_samples > 0 else 0
    
    # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ê³„ì‚°
    class_metrics = {}
    for class_idx in range(20):
        tp = true_positives[class_idx]
        fp = false_positives[class_idx]
        fn = false_negatives[class_idx]
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        class_metrics[class_idx] = {
            'class_name': VOC_CLASSES[class_idx],
            'precision': precision,
            'recall': recall,
            'f1_score': f1_score,
            'true_positives': tp,
            'false_positives': fp,
            'false_negatives': fn,
            'predictions': class_predictions[class_idx],
            'ground_truth': class_ground_truth[class_idx]
        }
    
    # í‰ê·  ë©”íŠ¸ë¦­ ê³„ì‚°
    avg_precision = np.mean([metrics['precision'] for metrics in class_metrics.values()])
    avg_recall = np.mean([metrics['recall'] for metrics in class_metrics.values()])
    avg_f1_score = np.mean([metrics['f1_score'] for metrics in class_metrics.values()])
    
    # ì˜ˆì¸¡ ë‹¤ì–‘ì„± ë¶„ì„
    unique_predictions = len(class_predictions)
    max_entropy = np.log(20)  # 20ê°œ í´ë˜ìŠ¤ì— ëŒ€í•œ ìµœëŒ€ ì—”íŠ¸ë¡œí”¼
    pred_counts = np.array(list(class_predictions.values()))
    pred_probs = pred_counts / np.sum(pred_counts)
    entropy = -np.sum(pred_probs * np.log(pred_probs + 1e-8))
    
    # í¸í–¥ ë¶„ì„
    aeroplane_idx = VOC_CLASSES.index('aeroplane')
    aeroplane_predictions = class_predictions.get(aeroplane_idx, 0)
    aeroplane_bias = (aeroplane_predictions / total_samples * 100) if total_samples > 0 else 0
    
    return {
        'overall_accuracy': overall_accuracy,
        'total_samples': total_samples,
        'correct_predictions': correct_predictions,
        'avg_precision': avg_precision,
        'avg_recall': avg_recall,
        'avg_f1_score': avg_f1_score,
        'class_metrics': class_metrics,
        'prediction_diversity': {
            'unique_classes': unique_predictions,
            'entropy': entropy,
            'max_entropy': max_entropy,
            'diversity_ratio': entropy / max_entropy
        },
        'bias_analysis': {
            'aeroplane_bias': aeroplane_bias,
            'aeroplane_predictions': aeroplane_predictions,
            'most_predicted_class': max(class_predictions, key=class_predictions.get) if class_predictions else None,
            'prediction_distribution': dict(class_predictions)
        },
        'confidence_stats': {
            'mean_confidence': np.mean(prediction_confidences),
            'std_confidence': np.std(prediction_confidences),
            'min_confidence': np.min(prediction_confidences),
            'max_confidence': np.max(prediction_confidences)
        }
    }

def print_detailed_results(results):
    """ìƒì„¸ ê²°ê³¼ ì¶œë ¥"""
    print("\n" + "="*80)
    print("ğŸ“Š FROM SCRATCH R-CNN ì •í™•í•œ ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼")
    print("="*80)
    
    # ì „ì²´ ì„±ëŠ¥
    print(f"\nğŸ¯ ì „ì²´ ì„±ëŠ¥:")
    print(f"  ğŸ“Š ì •í™•ë„: {results['overall_accuracy']:.2f}% ({results['correct_predictions']}/{results['total_samples']})")
    print(f"  ğŸ“ˆ í‰ê·  ì •ë°€ë„: {results['avg_precision']:.4f}")
    print(f"  ğŸ“‰ í‰ê·  ì¬í˜„ìœ¨: {results['avg_recall']:.4f}")
    print(f"  ğŸ¯ í‰ê·  F1-score: {results['avg_f1_score']:.4f}")
    
    # ì˜ˆì¸¡ ë‹¤ì–‘ì„±
    diversity = results['prediction_diversity']
    print(f"\nğŸŒˆ ì˜ˆì¸¡ ë‹¤ì–‘ì„±:")
    print(f"  ğŸ“Š ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ ìˆ˜: {diversity['unique_classes']}/20")
    print(f"  ğŸ“ˆ ì—”íŠ¸ë¡œí”¼: {diversity['entropy']:.3f}/{diversity['max_entropy']:.3f}")
    print(f"  ğŸ“Š ë‹¤ì–‘ì„± ë¹„ìœ¨: {diversity['diversity_ratio']:.3f}")
    
    # í¸í–¥ ë¶„ì„
    bias = results['bias_analysis']
    print(f"\nâš–ï¸ í¸í–¥ ë¶„ì„:")
    print(f"  âœˆï¸ Aeroplane í¸í–¥: {bias['aeroplane_bias']:.1f}% ({bias['aeroplane_predictions']}íšŒ)")
    if bias['aeroplane_bias'] == 100.0:
        print("  âŒ ì‹¬ê°í•œ í¸í–¥: 100% aeroplane ì˜ˆì¸¡")
    elif bias['aeroplane_bias'] > 50.0:
        print("  âš ï¸ ë†’ì€ í¸í–¥: >50% aeroplane ì˜ˆì¸¡")
    elif bias['aeroplane_bias'] > 20.0:
        print("  âš ï¸ ì¤‘ê°„ í¸í–¥: >20% aeroplane ì˜ˆì¸¡")
    else:
        print("  âœ… í¸í–¥ í•´ê²°: ì •ìƒì  ì˜ˆì¸¡ ë¶„í¬")
    
    # ì‹ ë¢°ë„ í†µê³„
    conf = results['confidence_stats']
    print(f"\nğŸ“Š ì˜ˆì¸¡ ì‹ ë¢°ë„:")
    print(f"  ğŸ“ˆ í‰ê· : {conf['mean_confidence']:.4f}")
    print(f"  ğŸ“Š í‘œì¤€í¸ì°¨: {conf['std_confidence']:.4f}")
    print(f"  ğŸ“‰ ìµœì†Œ: {conf['min_confidence']:.4f}")
    print(f"  ğŸ“ˆ ìµœëŒ€: {conf['max_confidence']:.4f}")
    
    # ìƒìœ„ 5ê°œ í´ë˜ìŠ¤ ì„±ëŠ¥
    print(f"\nğŸ† í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ (ìƒìœ„ 5ê°œ):")
    sorted_classes = sorted(results['class_metrics'].items(), 
                          key=lambda x: x[1]['f1_score'], reverse=True)
    
    print(f"{'ìˆœìœ„':<4} {'í´ë˜ìŠ¤':<12} {'ì •ë°€ë„':<8} {'ì¬í˜„ìœ¨':<8} {'F1-Score':<8} {'ì˜ˆì¸¡ìˆ˜':<6}")
    print("-" * 60)
    
    for i, (class_idx, metrics) in enumerate(sorted_classes[:5]):
        print(f"{i+1:<4} {metrics['class_name']:<12} "
              f"{metrics['precision']:<8.3f} {metrics['recall']:<8.3f} "
              f"{metrics['f1_score']:<8.3f} {metrics['predictions']:<6}")

def save_results_to_report(results, model_name):
    """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    report = {
        'model_name': model_name,
        'evaluation_time': timestamp,
        'performance': {
            'accuracy': results['overall_accuracy'],
            'avg_precision': results['avg_precision'],
            'avg_recall': results['avg_recall'],
            'avg_f1_score': results['avg_f1_score']
        },
        'detailed_results': results
    }
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    with open('from_scratch_accurate_results.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    # í…ìŠ¤íŠ¸ ë³´ê³ ì„œ ì €ì¥
    with open('from_scratch_accurate_report.txt', 'w', encoding='utf-8') as f:
        f.write(f"FROM SCRATCH R-CNN ì •í™•í•œ ì„±ëŠ¥ ì¸¡ì • ë³´ê³ ì„œ\n")
        f.write(f"="*60 + "\n")
        f.write(f"ëª¨ë¸: {model_name}\n")
        f.write(f"í‰ê°€ ì‹œê°„: {timestamp}\n")
        f.write(f"í‰ê°€ ìƒ˜í”Œ ìˆ˜: {results['total_samples']}\n\n")
        
        f.write(f"ì „ì²´ ì„±ëŠ¥:\n")
        f.write(f"  ì •í™•ë„: {results['overall_accuracy']:.2f}%\n")
        f.write(f"  í‰ê·  ì •ë°€ë„: {results['avg_precision']:.4f}\n")
        f.write(f"  í‰ê·  ì¬í˜„ìœ¨: {results['avg_recall']:.4f}\n")
        f.write(f"  í‰ê·  F1-score: {results['avg_f1_score']:.4f}\n\n")
        
        f.write(f"í¸í–¥ ë¶„ì„:\n")
        f.write(f"  Aeroplane í¸í–¥: {results['bias_analysis']['aeroplane_bias']:.1f}%\n")
        f.write(f"  ì˜ˆì¸¡ ë‹¤ì–‘ì„±: {results['prediction_diversity']['unique_classes']}/20 í´ë˜ìŠ¤\n\n")
        
        f.write(f"í´ë˜ìŠ¤ë³„ ì„±ëŠ¥:\n")
        for class_idx, metrics in results['class_metrics'].items():
            f.write(f"  {metrics['class_name']}: P={metrics['precision']:.3f}, "
                   f"R={metrics['recall']:.3f}, F1={metrics['f1_score']:.3f}\n")
    
    print(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
    print(f"  ğŸ“„ JSON: from_scratch_accurate_results.json")
    print(f"  ğŸ“ í…ìŠ¤íŠ¸: from_scratch_accurate_report.txt")

def main():
    """ë©”ì¸ í‰ê°€ í•¨ìˆ˜"""
    print("ğŸ¯ FROM SCRATCH R-CNN ì •í™•í•œ ì„±ëŠ¥ ì¸¡ì • ì‹œì‘")
    print("="*60)
    
    # í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ë“¤
    models_to_test = [
        ('models/test_model_from_scratch.pth', 'From Scratch Model'),
        ('models/completely_debiased_model.pth', 'Debiased From Scratch'),
        ('models/improved_best_model.pth', 'Improved From Scratch')
    ]
    
    all_results = {}
    
    for model_path, model_name in models_to_test:
        if os.path.exists(model_path):
            print(f"\nğŸ”„ í‰ê°€ ì¤‘: {model_name}")
            print("-" * 40)
            
            try:
                # ëª¨ë¸ ë¡œë“œ
                model, device = load_from_scratch_model(model_path)
                
                # ì¢…í•© í‰ê°€ ì‹¤í–‰
                results = evaluate_model_comprehensive(model, device, sample_size=1000)
                
                # ê²°ê³¼ ì¶œë ¥
                print_detailed_results(results)
                
                # ê²°ê³¼ ì €ì¥
                save_results_to_report(results, model_name)
                
                all_results[model_name] = results
                
            except Exception as e:
                print(f"âŒ Error evaluating {model_name}: {e}")
        else:
            print(f"âš ï¸ Model not found: {model_path}")
    
    # ëª¨ë¸ ë¹„êµ (ì—¬ëŸ¬ ëª¨ë¸ì´ ìˆëŠ” ê²½ìš°)
    if len(all_results) > 1:
        print(f"\nğŸ”„ ëª¨ë¸ ë¹„êµ ìš”ì•½:")
        print("="*80)
        
        for model_name, results in all_results.items():
            print(f"\n{model_name}:")
            print(f"  ì •í™•ë„: {results['overall_accuracy']:.2f}%")
            print(f"  ì˜ˆì¸¡ ë‹¤ì–‘ì„±: {results['prediction_diversity']['unique_classes']}/20")
            print(f"  Aeroplane í¸í–¥: {results['bias_analysis']['aeroplane_bias']:.1f}%")

if __name__ == "__main__":
    main() 