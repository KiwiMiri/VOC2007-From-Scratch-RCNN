#!/usr/bin/env python3
"""
í¸í–¥ ë¬¸ì œ ì™„ì „ í•´ê²° ìŠ¤í¬ë¦½íŠ¸
- ë‹¤ì–‘í•œ ì´ˆê¸°í™” ë°©ë²• ì‹œë„
- ê°€ì¤‘ì¹˜ ë¶„ì„ ë° ìˆ˜ì •
- ê·¹ë‹¨ì  bias ì œê±°
"""

import os
import sys
import torch
import torch.nn as nn
import numpy as np
from collections import Counter

sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from models.rcnn import RCNN

VOC_CLASSES = [
    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat',
    'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person',
    'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'
]

def test_bias(model, device, test_name=""):
    """í¸í–¥ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print(f"\nğŸ§ª Testing {test_name}")
    print("-" * 40)
    
    # ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ì…ë ¥
    test_inputs = [
        torch.randn(1, 3, 224, 224).to(device),
        torch.zeros(1, 3, 224, 224).to(device),
        torch.ones(1, 3, 224, 224).to(device),
        torch.randn(1, 3, 224, 224).to(device) * 0.5,
        torch.randn(1, 3, 224, 224).to(device) * 2.0,
    ]
    
    predictions = []
    
    model.eval()
    with torch.no_grad():
        for test_input in test_inputs:
            cls_scores, _ = model(test_input)
            predicted_class = torch.argmax(cls_scores, dim=1).item()
            predictions.append(predicted_class)
    
    # ì˜ˆì¸¡ ë¶„í¬ ë¶„ì„
    pred_counter = Counter(predictions)
    max_count = max(pred_counter.values()) if pred_counter else 0
    bias_percentage = (max_count / len(predictions)) * 100
    
    print(f"Prediction distribution:")
    for class_idx, count in pred_counter.most_common():
        if class_idx < len(VOC_CLASSES):
            name = VOC_CLASSES[class_idx]
        elif class_idx == len(VOC_CLASSES):
            name = "BACKGROUND"
        else:
            name = f"ERROR_{class_idx}"
        percentage = (count / len(predictions)) * 100
        print(f"  {name}: {count}/5 ({percentage:.1f}%)")
    
    print(f"Max bias: {bias_percentage:.1f}%")
    return bias_percentage, len(pred_counter)

def apply_aggressive_debiasing(model):
    """ê³µê²©ì ì¸ í¸í–¥ ì œê±°"""
    print("ğŸ”§ Applying aggressive debiasing...")
    
    # 1. ë¶„ë¥˜ê¸°ì˜ ë§ˆì§€ë§‰ ë ˆì´ì–´ biasë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”
    if hasattr(model.classifier[-1], 'bias') and model.classifier[-1].bias is not None:
        nn.init.constant_(model.classifier[-1].bias, 0.0)
        print("  âœ… Reset final classifier bias to zero")
    
    # 2. ë§ˆì§€ë§‰ ë ˆì´ì–´ ê°€ì¤‘ì¹˜ë¥¼ ë§¤ìš° ì‘ì€ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
    if hasattr(model.classifier[-1], 'weight'):
        nn.init.normal_(model.classifier[-1].weight, mean=0.0, std=0.001)
        print("  âœ… Reset final classifier weights (std=0.001)")
    
    # 3. ëª¨ë“  ë¶„ë¥˜ê¸° bias ì¬ì„¤ì •
    for module in model.classifier.modules():
        if isinstance(module, nn.Linear) and module.bias is not None:
            nn.init.constant_(module.bias, 0.0)
    
    print("  âœ… Reset all classifier biases")

def apply_uniform_initialization(model):
    """ê· ë“± ì´ˆê¸°í™” ì ìš©"""
    print("ğŸ”§ Applying uniform initialization...")
    
    for module in model.classifier.modules():
        if isinstance(module, nn.Linear):
            # ê· ë“± ë¶„í¬ë¡œ ì´ˆê¸°í™”
            nn.init.uniform_(module.weight, -0.1, 0.1)
            if module.bias is not None:
                nn.init.uniform_(module.bias, -0.01, 0.01)
    
    print("  âœ… Applied uniform initialization")

def apply_he_initialization(model):
    """He ì´ˆê¸°í™” ì ìš©"""
    print("ğŸ”§ Applying He initialization...")
    
    for module in model.classifier.modules():
        if isinstance(module, nn.Linear):
            nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
            if module.bias is not None:
                nn.init.constant_(module.bias, 0.0)
    
    print("  âœ… Applied He initialization")

def fix_bias_completely():
    """í¸í–¥ ë¬¸ì œ ì™„ì „ í•´ê²°"""
    print("ğŸ¯ COMPLETE BIAS ELIMINATION")
    print("=" * 60)
    
    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # ì›ë³¸ ëª¨ë¸ ë¡œë“œ
    model_path = 'models/fixed_existing_model.pth'
    model = RCNN(num_classes=20, pretrained=False)
    checkpoint = torch.load(model_path, map_location=device)
    
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint)
    
    model = model.to(device)
    
    # ì›ë³¸ í¸í–¥ í…ŒìŠ¤íŠ¸
    bias_before, diversity_before = test_bias(model, device, "Original Model")
    
    # ë‹¤ì–‘í•œ í•´ê²° ë°©ë²• ì‹œë„
    methods = [
        ("Aggressive Debiasing", apply_aggressive_debiasing),
        ("Uniform Initialization", apply_uniform_initialization),
        ("He Initialization", apply_he_initialization),
    ]
    
    best_bias = bias_before
    best_diversity = diversity_before
    best_method = "Original"
    best_model_state = None
    
    for method_name, method_func in methods:
        # ëª¨ë¸ ìƒíƒœ ë°±ì—…
        original_state = model.state_dict().copy()
        
        # ë°©ë²• ì ìš©
        method_func(model)
        
        # í¸í–¥ í…ŒìŠ¤íŠ¸
        bias, diversity = test_bias(model, device, method_name)
        
        # ì„±ëŠ¥ í‰ê°€ (ë‚®ì€ í¸í–¥ + ë†’ì€ ë‹¤ì–‘ì„±ì´ ì¢‹ìŒ)
        score = (100 - bias) + (diversity * 10)  # ë‹¤ì–‘ì„±ì— ê°€ì¤‘ì¹˜
        best_score = (100 - best_bias) + (best_diversity * 10)
        
        if score > best_score:
            best_bias = bias
            best_diversity = diversity
            best_method = method_name
            best_model_state = model.state_dict().copy()
            print(f"  ğŸ† New best method: {method_name}")
        
        # ì›ë³¸ ìƒíƒœ ë³µì› (ë‹¤ìŒ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´)
        model.load_state_dict(original_state)
    
    # ìµœê³  ì„±ëŠ¥ ë°©ë²• ì ìš©
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        print(f"\nğŸ† Best method: {best_method}")
        print(f"   Bias: {best_bias:.1f}% (was {bias_before:.1f}%)")
        print(f"   Diversity: {best_diversity}/20 classes (was {diversity_before}/20)")
    
    # ì¶”ê°€ ë¯¸ì„¸ ì¡°ì •
    if best_bias > 50:  # ì—¬ì „íˆ í¸í–¥ì´ ìˆë‹¤ë©´
        print("\nğŸ”§ Applying additional fine-tuning...")
        
        # ê·¹ë‹¨ì  ì¡°ì¹˜: ë§ˆì§€ë§‰ ë ˆì´ì–´ ì™„ì „ ì¬ì´ˆê¸°í™”
        if hasattr(model.classifier[-1], 'weight'):
            fan_in = model.classifier[-1].weight.size(1)
            std = np.sqrt(2.0 / fan_in)  # He initialization std
            
            # ë§¤ìš° ì‘ì€ ê°€ì¤‘ì¹˜ë¡œ ì´ˆê¸°í™”
            nn.init.normal_(model.classifier[-1].weight, mean=0.0, std=std * 0.1)
            if model.classifier[-1].bias is not None:
                nn.init.normal_(model.classifier[-1].bias, mean=0.0, std=0.001)
        
        # ìµœì¢… í…ŒìŠ¤íŠ¸
        final_bias, final_diversity = test_bias(model, device, "Final Debiased Model")
        
        print(f"\nğŸ“Š Final Results:")
        print(f"   Before: {bias_before:.1f}% bias, {diversity_before} classes")
        print(f"   After:  {final_bias:.1f}% bias, {final_diversity} classes")
        
        improvement = bias_before - final_bias
        print(f"   Improvement: {improvement:.1f}% reduction in bias")
    
    # ëª¨ë¸ ì €ì¥
    output_path = 'models/completely_debiased_model.pth'
    
    save_dict = {
        'model_state_dict': model.state_dict(),
        'method_used': best_method,
        'bias_before': bias_before,
        'bias_after': best_bias,
        'diversity_before': diversity_before,
        'diversity_after': best_diversity,
        'epoch': checkpoint.get('epoch', 'Unknown'),
    }
    
    torch.save(save_dict, output_path)
    print(f"\nğŸ’¾ Completely debiased model saved: {output_path}")
    
    # ì„±ê³µ ì—¬ë¶€ íŒì •
    if best_bias < 40:  # 40% ë¯¸ë§Œì´ë©´ ì„±ê³µ
        print("\nğŸ‰ SUCCESS: Bias significantly reduced!")
    elif best_bias < 60:
        print("\nâš ï¸ PARTIAL SUCCESS: Some bias reduction achieved")
    else:
        print("\nâŒ FAILED: Bias still too high")
        print("   May need architectural changes or more training data")
    
    return model

if __name__ == "__main__":
    fixed_model = fix_bias_completely() 